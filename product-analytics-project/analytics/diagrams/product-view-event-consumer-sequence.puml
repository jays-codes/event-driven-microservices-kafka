@startuml Product Analytics Sequence Diagram

title Product Analytics - Complete Architecture Flow

participant "Client" as Client
participant "TrendingController" as Controller
participant "TrendingProductsBroadcastService" as Broadcast
participant "Kafka Topic<br/>(product-view-events)" as Kafka
participant "ReactiveKafkaConsumerTemplate" as Template
participant "ProductViewEventConsumer" as Consumer
participant "ProductViewAnalyticsService" as Analytics
participant "Sinks.Many<br/>(Event Signal)" as Sink
participant "ProductViewRepository" as Repository
participant "Database<br/>(H2)" as DB

== System Initialization ==
Consumer -> Template: subscribe() @PostConstruct
activate Consumer
Template -> Template: receive()
note right: Creates reactive stream<br/>of ReceiverRecord messages

Broadcast -> Broadcast: init() @PostConstruct
activate Broadcast
Broadcast -> Repository: findTop5ByOrderByCountDesc()
activate Repository
Repository -> DB: SELECT * FROM product_view_count<br/>ORDER BY count DESC LIMIT 5
activate DB
DB -> Repository: Top 5 ProductViewCount records
deactivate DB
Repository -> Broadcast: Flux<ProductViewCount>
deactivate Repository

Broadcast -> Analytics: getCompanionFlux()
Broadcast -> Broadcast: map to ProductTrendingDTO<br/>collectList(), repeatWhen(companionFlux)<br/>distinctUntilChanged(), cache(1)
note right: **Event-Driven Stream Setup:**<br/>• Triggered by data changes<br/>• Uses companion flux from Analytics<br/>• Caches latest result<br/>• Only emits when changed
deactivate Broadcast

== Write Path: Event Processing ==
loop Continuous Kafka Event Processing
    Kafka -> Template: ProductViewEvent messages
    activate Template
    Template -> Template: bufferTimeout(1000, Duration.ofSeconds(1))
    note right: **Batching Strategy:**<br/>• Collect up to 1000 messages<br/>• OR wait max 1 second<br/>• Whichever comes first
    
    Template -> Consumer: Batched events
    Consumer -> Analytics: processBatch(List<ReceiverRecord<String, ProductViewEvent>>)
    activate Analytics
    
    == Event Aggregation ==
    Analytics -> Analytics: events.stream()<br/>.map(r -> r.value().getProductId())<br/>.collect(groupingBy(identity(), counting()))
    note right: **Creates evtMap:**<br/>{1: 3, 2: 1, 5: 2}
    
    == Database Lookup & Update ==
    Analytics -> Repository: findAllById(evtMap.keySet())
    activate Repository
    Repository -> DB: SELECT * FROM product_view_count<br/>WHERE id IN (1, 2, 5)
    activate DB
    DB -> Repository: Existing ProductViewCount records
    deactivate DB
    Repository -> Analytics: Flux<ProductViewCount>
    deactivate Repository
    
    Analytics -> Analytics: collectMap(ProductViewCount::getId)<br/>defaultIfEmpty(emptyMap())
    
    loop For each productId in batch
        Analytics -> Analytics: updateViewCount(dbMap, evtMap, productId)
        note right: **Upsert Logic:**<br/>• Update existing counts<br/>• Create new records<br/>• Set isNew=false
    end
    
    == Database Persistence ==
    Analytics -> Repository: saveAll(List<ProductViewCount>)
    activate Repository
    Repository -> DB: Batch INSERT/UPDATE operations
    activate DB
    DB -> DB: Upsert product view counts
    deactivate DB
    Repository -> Analytics: Saved entities
    deactivate Repository
    
    == Kafka Acknowledgment & Event Signal ==
    Analytics -> Analytics: doOnComplete(() -> acknowledge())
    note right: **Acknowledge last message**<br/>commits entire batch offset
    
    Analytics -> Sink: tryEmitNext(1)
    activate Sink
    note right: **Signal Data Change**<br/>Triggers companion flux<br/>for real-time updates
    Sink -> Broadcast: Signal propagation
    deactivate Sink
    
    Analytics -> Consumer: Mono<Void> (completion)
    deactivate Analytics
    Consumer -> Template: Processing complete
    Template -> Template: Subscribe to next batch
    deactivate Template
end

== Read Path: Real-Time Trending Data ==
Client -> Controller: GET /trending (SSE request)
activate Controller
note right: **Server-Sent Events**<br/>Content-Type: text/event-stream

Controller -> Broadcast: getTrends()
activate Broadcast
Broadcast -> Broadcast: Return cached trending stream
note right: **Cached Stream Benefits:**<br/>• No database hit per request<br/>• Shared among all clients<br/>• Event-driven updates

Broadcast -> Controller: Flux<List<ProductTrendingDTO>>
deactivate Broadcast

Controller -> Client: Streaming trending data
note left: **Real-Time Updates:**<br/>• Live data stream via SSE<br/>• Immediate updates on data changes<br/>• Only when data changes<br/>• Multiple clients supported

== Background: Event-Driven Data Refresh ==
note over Sink, DB: **Triggered by data changes (event-driven)**
Sink -> Broadcast: Companion flux signal
activate Broadcast
Broadcast -> Repository: findTop5ByOrderByCountDesc()
activate Repository
Repository -> DB: SELECT top 5 trending products
activate DB
DB -> Repository: Latest trending data
deactivate DB
Repository -> Broadcast: Fresh ProductViewCount data
deactivate Repository
Broadcast -> Broadcast: distinctUntilChanged()<br/>Only emit if data changed
Broadcast -> Controller: Updated trending data (if changed)
Controller -> Client: New trending data via SSE
deactivate Broadcast

deactivate Controller

== Architecture Benefits ==
note over Client, DB
  **Enhanced Event-Driven CQRS Architecture:**
  
  **Write Side (Command):**
  • Event-driven via Kafka
  • Batch processing for efficiency
  • Handles high-volume product view events
  • Emits signals on data changes
  
  **Read Side (Query):**
  • Event-driven data refresh
  • Immediate response to changes
  • Cached for performance
  • Real-time streaming to clients
  
  **Benefits:**
  • Zero polling delays: Instant updates
  • Efficient: Updates only when needed
  • Reactive: End-to-end event streaming
  • Scalable: Independent read/write paths
  • Responsive: Real-time user experience
end note

@enduml